{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyserini'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlucene\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleIndexer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyserini\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlucene\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LuceneSearcher\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyserini'"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "import json \n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load your embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def truncate_text_to_bytes(text, max_bytes=40960):\n",
    "    \"\"\"Truncate text to fit within a specified byte limit.\"\"\"\n",
    "    # Start with an initial truncation within a reasonable character limit\n",
    "    truncated_text = text[:15000]\n",
    "    # Iteratively reduce size if byte count exceeds the limit\n",
    "    while len(truncated_text.encode('utf-8')) > max_bytes:\n",
    "        truncated_text = truncated_text[:-100]  # Remove more characters\n",
    "    return truncated_text\n",
    "\n",
    "def load_and_embed_dataset(dataset_path, prompt_field=\"prompt\", completion_field=\"completion\"):\n",
    "    \"\"\"\n",
    "    Load dataset from JSONL, truncate metadata to fit byte limits, and embed texts.\n",
    "    Args:\n",
    "        dataset_path (str): Path to the JSONL file with \"prompt\" and \"completion\".\n",
    "        prompt_field (str): Field name for prompt.\n",
    "        completion_field (str): Field name for completion.\n",
    "    Returns:\n",
    "        embeddings (np.ndarray): Embedded vectors.\n",
    "        metadata (list): List of dictionaries with truncated \"prompt\" and \"completion\".\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset and embedding texts...\")\n",
    "\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Get and truncate prompt and completion to fit within metadata size limits\n",
    "            prompt = truncate_text_to_bytes(data.get(prompt_field, \"\"))\n",
    "            completion = truncate_text_to_bytes(data.get(completion_field, \"\"))\n",
    "\n",
    "            # Concatenate prompt and completion for embedding\n",
    "            text_to_embed = f\"{prompt} {completion}\"\n",
    "            embedding = model.encode(text_to_embed)\n",
    "\n",
    "            # Append embedding and metadata to lists\n",
    "            embeddings.append(embedding)\n",
    "            metadata.append({\"prompt\": prompt, \"completion\": completion})\n",
    "\n",
    "    # Convert embeddings list to numpy array\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(\"Embedding and truncation complete.\")\n",
    "\n",
    "    return embeddings, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pinecone_index(index_name: str, dimension: int, metric: str = 'cosine'):\n",
    "    print(\"Creating a Pinecone index...\")\n",
    "    pc = Pinecone(api_key=\"35b290d2-dcfb-4fde-b395-cc19f1e5aadd\")\n",
    "    existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "    if index_name not in existing_indexes:\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=dimension,\n",
    "            metric=metric,\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "    print(\"Done!\")\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyserini.index.lucene import SimpleIndexer\n",
    "# from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "# # Add sparse indexing function\n",
    "# def index_sparse_with_pyserini(jsonl_file, sparse_index_path, batch_size=1000):\n",
    "#     \"\"\"\n",
    "#     Index documents for sparse retrieval (BM25) using Lucene (Pyserini).\n",
    "    \n",
    "#     Args:\n",
    "#         jsonl_file: Path to the JSONL file containing documents.\n",
    "#         sparse_index_path: Directory path for the Lucene sparse index.\n",
    "#         batch_size: Number of documents to process in each batch.\n",
    "#     \"\"\"\n",
    "#     os.makedirs(sparse_index_path, exist_ok=True)\n",
    "#     indexer = SimpleIndexer(sparse_index_path)\n",
    "    \n",
    "#     with open(jsonl_file, 'r') as f:\n",
    "#         for line in f:\n",
    "#             entry = json.loads(line)\n",
    "#             prompt = entry.get(\"prompt\", \"\")\n",
    "#             completion = entry.get(\"completion\", \"\")\n",
    "#             document = f\"Prompt: {prompt}\\nCompletion: {completion}\"\n",
    "#             doc_id = f\"{hash(prompt)}_{hash(completion)}\"\n",
    "#             indexer.add_document(doc_id, document)\n",
    "    \n",
    "#     indexer.close()\n",
    "#     print(\"Sparse indexing with BM25 completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_vectors(\n",
    "        index: Pinecone,\n",
    "        embeddings: np.ndarray,\n",
    "        dataset: list,\n",
    "        prompt_field: str = 'prompt',\n",
    "        completion_field: str = 'completion',\n",
    "        batch_size: int = 128\n",
    "):\n",
    "    \"\"\"\n",
    "    Upsert vectors to a Pinecone index with metadata from a custom dataset.\n",
    "    \n",
    "    Args:\n",
    "        index: The Pinecone index object\n",
    "        embeddings: The embeddings to upsert (NumPy array)\n",
    "        dataset: The dataset list, where each item is a dictionary with prompt and completion\n",
    "        prompt_field: The field name for the prompt text in each data item\n",
    "        completion_field: The field name for the completion text in each data item\n",
    "        batch_size: The batch size to use for upserting\n",
    "    \n",
    "    Returns:\n",
    "        The updated Pinecone index\n",
    "    \"\"\"\n",
    "    print(\"Upserting the embeddings to the Pinecone index...\")\n",
    "\n",
    "    # Generate unique IDs for each embedding\n",
    "    ids = [str(i) for i in range(len(embeddings))]\n",
    "    \n",
    "    # Create metadata by extracting prompt and completion fields from each item in the dataset\n",
    "    meta = [\n",
    "        {prompt_field: item[prompt_field], completion_field: item[completion_field]} \n",
    "        for item in dataset\n",
    "    ]\n",
    "    \n",
    "    # Prepare list of (id, vector, metadata) tuples for upsert\n",
    "    to_upsert = list(zip(ids, embeddings, meta))\n",
    "    \n",
    "    # Upsert in batches\n",
    "    for i in tqdm(range(0, len(embeddings), batch_size)):\n",
    "        i_end = min(i + batch_size, len(embeddings))\n",
    "        index.upsert(vectors=to_upsert[i:i_end])\n",
    "    \n",
    "    print(\"Upsert completed.\")\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Trump_DB.jsonl'\n",
    "index_name = \"trumpdb\"  # Ensure this name is lowercase and uses only letters, numbers, or dashes\n",
    "embeddings, metadata = load_and_embed_dataset(file_path)\n",
    "\n",
    "print(\"done loading and embed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = embeddings.shape[1]  # Adjust to match the dimension of your embeddings\n",
    "pc = create_pinecone_index(index_name, dimension)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch \n",
    "from trump_agent import TrumpAgent\n",
    "from biden_agent import BidenAgent\n",
    "from eval_agent import EvalAgent\n",
    "# Load tokenizer and model for embedding if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def load_debate_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        debate_data = json.load(file)\n",
    "    return debate_data\n",
    "\n",
    "def embed_text(text):\n",
    "    # Tokenize and embed the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).pooler_output  # use the pooled output as the embedding\n",
    "    return embeddings[0].cpu().numpy()  # convert to numpy array\n",
    "\n",
    "def query_index(question, index, top_k=3):\n",
    "    # Generate the embedding for the query question\n",
    "    query_embedding = embed_text(question)\n",
    "    \n",
    "    # Perform the query with Pinecone\n",
    "    try:\n",
    "        response = index.query(vector=query_embedding.tolist(), top_k=top_k, include_metadata=True)\n",
    "        # Extract the relevant metadata, which includes the \"completion\" field\n",
    "        top_completions = [match['metadata']['completion'] for match in response['matches']]\n",
    "    except AttributeError as e:\n",
    "        print(f\"Error querying index: {e}\")\n",
    "        top_completions = []\n",
    "\n",
    "    return top_completions\n",
    "\n",
    "\n",
    "\n",
    "def generate_debate_response(question, trump_agent):\n",
    "    # Query Pinecone to retrieve relevant context for the question\n",
    "    top_completions = query_index(question, index)\n",
    "    \n",
    "    # Combine retrieved completions as context for the Trump agent\n",
    "    context = \"\\n\".join(top_completions)\n",
    "    \n",
    "    # Generate Trump agent response with the context\n",
    "    trump_response = trump_agent.generate_response(context)\n",
    "    return trump_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def query_index(question, index, sparse_searcher, top_k=3, dense_weight=0.5, sparse_weight=0.5):\n",
    "#     \"\"\"\n",
    "#     Perform a hybrid search using both Pinecone dense retrieval and Pyserini sparse retrieval.\n",
    "    \n",
    "#     Args:\n",
    "#         question: Query question string.\n",
    "#         index: Pinecone dense index instance.\n",
    "#         sparse_searcher: Pyserini sparse searcher instance (BM25).\n",
    "#         top_k: Number of top results to retrieve.\n",
    "#         dense_weight: Weight for dense retrieval scores.\n",
    "#         sparse_weight: Weight for sparse retrieval scores.\n",
    "    \n",
    "#     Returns:\n",
    "#         List of top completions based on hybrid scoring.\n",
    "#     \"\"\"\n",
    "#     # Dense retrieval with Pinecone\n",
    "#     query_embedding = embed_text(question)\n",
    "#     dense_response = index.query(vector=query_embedding.tolist(), top_k=top_k, include_metadata=True)\n",
    "#     dense_results = {match['id']: dense_weight * match['score'] for match in dense_response['matches']}\n",
    "    \n",
    "#     # Sparse retrieval with Pyserini\n",
    "#     sparse_hits = sparse_searcher.search(question, k=top_k)\n",
    "#     sparse_results = {hit.docid: sparse_weight * hit.score for hit in sparse_hits}\n",
    "    \n",
    "#     # Combine dense and sparse scores\n",
    "#     combined_scores = {}\n",
    "#     for doc_id, score in dense_results.items():\n",
    "#         combined_scores[doc_id] = combined_scores.get(doc_id, 0) + score\n",
    "#     for doc_id, score in sparse_results.items():\n",
    "#         combined_scores[doc_id] = combined_scores.get(doc_id, 0) + score\n",
    "    \n",
    "#     # Sort by combined score and retrieve top completions\n",
    "#     sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "#     top_completions = [sparse_searcher.doc(doc_id).raw() for doc_id, _ in sorted_docs]\n",
    "    \n",
    "#     return top_completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting the embeddings to the Pinecone index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:14<00:00,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsert completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x2a4638616a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsert_vectors(index=index, embeddings=embeddings, dataset=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4005}},\n",
       " 'total_vector_count': 4005}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question 1 for Biden...\n",
      "Generating response from Biden agent...\n",
      "\n",
      "Running model: gpt-4o-mini\n",
      "Biden Generated Response: {'gpt-4o-mini': 'Thank you for the question. Look, I understand the anxiety that so many families are feeling right now. I really do. When I walk through communities, when I talk to folks just like you, I hear the worries—grocery bills that are stretched thin, housing costs that seem to go up overnight. It’s a heavy burden, and it’s one that weighs on me every single day.\\n\\nWhen I took office, we faced unprecedented challenges. The pandemic left our economy in shambles, and many people were struggling just to make ends meet. But we rolled up our sleeves and got to work. The thing is, we didn’t just throw a blanket over these problems. The bipartisan Infrastructure Law, for instance, isn’t just about roads and bridges; it’s about creating good-paying jobs and ensuring that when you work hard, you can build a decent life for yourself and your family. And let’s not forget about the American Rescue Plan, which helped lift millions of children out of poverty and put money directly into the pockets of working families.\\n\\nNow, I know that inflation has hit hard, and I hear that frustration—believe me. But we’re making progress. Inflation has been coming down over the last few months, and we’re doing everything possible to stabilize prices. It’s about investing in our workers, in manufacturing, and in clean energy—things that not only strengthen our economy but help reduce costs in the long run.\\n\\nI was raised in a middle-class family, and I think about what it was like for my dad when he got laid off, figuring out how to put food on the table. We’ve got to prioritize American families—your families. We’re looking at ways to reduce costs for essential goods, extending healthcare coverage, and lowering prescription drug prices.\\n\\nI truly believe we can work together—across party lines—to tackle the challenges we face today. This is about unity, about coming together for the common good, because when middle-class families succeed, we all succeed. Let’s keep that at the forefront and work toward a future where every family can thrive, where a good meal is within reach, and where everyone has the opportunity to realize their dreams. Thank you.'}\n",
      "\n",
      "Evaluating response from: gpt-4o-mini\n",
      "\n",
      "Evaluation Feedback for Biden Response:\n",
      "{'gpt-4o-mini': 'I would rate the similarity between the generated response and the real response as a 4: Very similar. Here\\'s why:\\n\\n**Content Relevance:** The generated response covers many of the same topics as the real response, including the challenges faced by working-class families, the need to address inflation, and the importance of creating jobs and reducing costs for essential goods. Both responses also mention the president\\'s personal experience growing up in a middle-class family and the need to prioritize American families.\\n\\n**Stylistic Similarity:** The generated response uses similar language and phrasing to the real response, with a focus on storytelling and personal anecdotes. Both responses use colloquial expressions, such as \"kitchen table\" and \"put food on the table,\" to connect with everyday Americans. The tone of the generated response is also similar to the real response, with a mix of empathy, optimism, and determination.\\n\\n**Argument Alignment:** The generated response follows a similar line of reasoning as the real response, emphasizing the need to address the challenges faced by working-class families and to prioritize American families. Both responses also highlight the importance of investing in workers, manufacturing, and clean energy to strengthen the economy and reduce costs.\\n\\nHowever, there are some minor deviations between the two responses. For example, the generated response does not mention the specific policies mentioned in the real response, such as the bipartisan Infrastructure Law and the American Rescue Plan. Additionally, the generated response does not explicitly criticize the previous administration, as the real response does.\\n\\nOverall, while the generated response is not identical to the real response, it captures the essence of the president\\'s speaking style and argumentation, making it a very similar response.'}\n",
      "\n",
      "Processing question 2 for Trump...\n",
      "Generating response from Trump agent...\n",
      "\n",
      "Running model: gpt-4o-mini\n",
      "Trump Generated Response: {'gpt-4o-mini': 'Thank you! Look, folks, this economy has gone downhill, and it’s sad. Under my administration, we had the greatest economy in the history of our country—record low unemployment, wages were rising, and American jobs were booming! I brought back jobs! We brought back manufacturing! It was America First, and it worked like a charm.\\n\\nNow, what do we have? Under Biden, inflation is through the roof! Prices are skyrocketing. You go to the grocery store, and that basket of groceries that cost you $100? Now it’s over $120. That’s not just a number. That’s real money out of your pocket! Your hard-earned dollars are disappearing, folks. Home prices up 30 percent—they’re priced out of the American Dream!\\n\\nAnd let’s talk about energy—so important! Under my watch, we were energy independent. We were producing more oil than ever before. Now? Let’s just say we’re begging OPEC for help. It’s a disaster!\\n\\nWe need to get back to what works. We need to stop the spending spree, cut taxes, and unleash American businesses! We’re going to secure our border. We’re going to bring those jobs back from overseas. It’s all about American workers, American families, and making sure you’re not feeling the pinch every time you go shopping.\\n\\nSo, remember this: Under Trump, we thrived. Under Biden, we just survived. We’re going to make America great again—again! Thank you!'}\n",
      "\n",
      "Evaluating response from: gpt-4o-mini\n",
      "\n",
      "Evaluation Feedback for Trump Response:\n",
      "{'gpt-4o-mini': 'Based on the provided criteria, I would rate the similarity between the generated response and the real response as a 4: Very similar.\\n\\n**Content Relevance:** The generated response covers many of the same key points as the real response, including:\\n\\n* The claim of having a strong economy during their administration\\n* Criticism of the current administration\\'s handling of the economy, specifically regarding inflation\\n* Mention of job creation and employment\\n* Reference to energy independence\\n* Accusation of the current administration\\'s policies being detrimental to the country\\n\\n**Stylistic Similarity:** The generated response employs a similar tone and language characteristic of Trump\\'s speaking style, including:\\n\\n* Use of superlatives (\"the greatest economy in the history of our country\", \"through the roof\", \"disaster\")\\n* Emphasis on American exceptionalism (\"America First\", \"make America great again\")\\n* Use of rhetorical questions and exclamations (\"What do we have?\", \"Folks, it\\'s sad\")\\n* Reference to personal achievements and successes (\"Under my administration\", \"Under Trump, we thrived\")\\n\\n**Argument Alignment:** The generated response follows a similar line of reasoning as the real response, including:\\n\\n* Criticizing the current administration\\'s policies as ineffective and damaging to the country\\n* Emphasizing the importance of a strong economy and job creation\\n* Highlighting the contrast between their own administration\\'s achievements and the current administration\\'s failures\\n\\nHowever, there are some minor deviations in the generated response, such as the use of more hyperbolic language and the addition of new topics (e.g., energy independence). Nevertheless, the overall tone, style, and argumentation are very similar to the real response.'}\n",
      "\n",
      "Processing question 3 for Biden...\n",
      "Generating response from Biden agent...\n",
      "\n",
      "Running model: gpt-4o-mini\n",
      "Biden Generated Response: {'gpt-4o-mini': \"Thank you for the question. Look, folks, it’s important we talk honestly about health issues—especially during a time when so many families are worried about their own well-being. You know, my father always told me that it doesn't matter how tough things get; what matters is how we come together to support one another. That’s what we need to focus on today. \\n\\nNow, I hear what my opponent just said, but we’ve got to recognize that healthcare isn’t just about one person's experience or a short-term medication—it’s about building a system that works for every American. When I think about it, I remember a story—like that of a single mom I met in Pennsylvania. She works two jobs just to keep food on the table and, like many, she worries about paying for her child’s asthma medication. That’s the reality for so many families across this nation. \\n\\nWe’ve got to make healthcare affordable and accessible. During my administration, we expanded the Affordable Care Act, giving millions of Americans the security they need. I’m committed to lowering prescription drug prices and ensuring that health coverage includes mental health services, because it's just as important as physical health. \\n\\nAnd let’s be clear: at a time when our nation faces so many divisions, it's essential that we unite around common goals—like healthcare for all. This isn’t a Democratic or Republican issue; it’s an American issue. When we come together, we can lift each other up, and hopefully, make sure that every family can have access to the healthcare they need without fear of financial ruin. \\n\\nSo, let’s work together to build a better, healthier future for everyone. Thank you.\"}\n",
      "\n",
      "Evaluating response from: gpt-4o-mini\n",
      "\n",
      "Evaluation Feedback for Biden Response:\n",
      "{'gpt-4o-mini': \"I would rate the similarity between the generated response and the real response as a 1: Very dissimilar.\\n\\nHere's why:\\n\\n**Content Relevance:** The generated response does not cover any of the main points or topics mentioned in the real response. The real response discusses the economy, tax cuts, deficit, job losses, and military issues, whereas the generated response talks about healthcare, the Affordable Care Act, and mental health services. There is no overlap in content.\\n\\n**Stylistic Similarity:** The tone and language used in the generated response are quite different from the real response. The real response has a more confrontational and critical tone, while the generated response is more conciliatory and focused on unity. The language used is also more generic and lacks the specificity and detail of the real response.\\n\\n**Argument Alignment:** The generated response does not follow the same line of reasoning or argumentation as the real response. The real response is focused on criticizing the opponent's economic policies and highlighting the speaker's own accomplishments, whereas the generated response is focused on promoting a specific policy agenda (healthcare) and emphasizing the importance of unity.\\n\\nOverall, the generated response appears to be a generic, scripted response that does not engage with the specific topics or arguments raised in the real response.\"}\n",
      "\n",
      "Processing question 4 for Trump...\n",
      "Generating response from Trump agent...\n",
      "\n",
      "Running model: gpt-4o-mini\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m candidate\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrump\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating response from Trump agent...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m     generated_response \u001b[38;5;241m=\u001b[39m trump_agent\u001b[38;5;241m.\u001b[39mgenerate_response(\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContext for the question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieved context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretrieved_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m     )\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrump Generated Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_response)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Evaluate Trump's response\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97252\\OneDrive\\Desktop\\semester 8\\Lab\\Project\\base_agent.py:15\u001b[0m, in \u001b[0;36mBaseAgent.generate_response\u001b[1;34m(self, user_prompt)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_client\u001b[38;5;241m.\u001b[39mget_response(model_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_prompt, user_prompt)\n\u001b[0;32m     16\u001b[0m     response_dict[model_name] \u001b[38;5;241m=\u001b[39m response\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# print(f\"Response :\\n{response}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97252\\OneDrive\\Desktop\\semester 8\\Lab\\Project\\LLM_API.py:42\u001b[0m, in \u001b[0;36mChatModelClient.get_response\u001b[1;34m(self, model_name, system_prompt, user_prompt, stream)\u001b[0m\n\u001b[0;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcomplete(\n\u001b[0;32m     33\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m     34\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m update \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update\u001b[38;5;241m.\u001b[39mchoices:\n\u001b[0;32m     44\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m update\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\azure\\ai\\inference\\models\\_patch.py:208\u001b[0m, in \u001b[0;36mStreamingChatCompletions.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_models.StreamingChatCompletionsUpdate\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mempty() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done:\n\u001b[1;32m--> 208\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_next_block()\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mempty():\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\azure\\ai\\inference\\models\\_patch.py:217\u001b[0m, in \u001b[0;36mStreamingChatCompletions._read_next_block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    215\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Reading next block]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 217\u001b[0m     element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bytes_iterator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__next__\u001b[39m()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\azure\\core\\rest\\_http_response_impl.py:423\u001b[0m, in \u001b[0;36mHttpResponseImpl.iter_bytes\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_download_check()\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_download_generator(\n\u001b[0;32m    424\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    425\u001b[0m         pipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m         decompress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    427\u001b[0m     )\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py:177\u001b[0m, in \u001b[0;36mStreamDownloadGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m internal_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39minternal_response\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content_func)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1209\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_chunk(amt)\n\u001b[0;32m   1210\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   1211\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m )\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:1156\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returned_chunk\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\http\\client.py:640\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    642\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\97252\\anaconda3\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
